import { Edge } from '@xyflow/react';

export type AbstractionLevel = 'field' | 'theory' | 'method' | 'algorithm' | 'implementation';
export type Language = 'ru' | 'en';

export interface NodeContent {
  label: string;
  description: string;
  keyPoints: string[];
  howItWorks: string; // –ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞
}

export interface AINodeData {
  emoji?: string;
  level: AbstractionLevel;
  ru: NodeContent;
  en: NodeContent;
  [key: string]: unknown;
}

export interface AINode {
  id: string;
  position: { x: number; y: number };
  type: string;
  data: AINodeData;
}

// –¶–≤–µ—Ç–∞ –ø–æ —É—Ä–æ–≤–Ω—é –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏
export const levelColors: Record<AbstractionLevel, string> = {
  field: '#6366f1',
  theory: '#8b5cf6',
  method: '#06b6d4',
  algorithm: '#10b981',
  implementation: '#f59e0b',
};

export const levelLabels: Record<Language, Record<AbstractionLevel, string>> = {
  ru: {
    field: '–û–±–ª–∞—Å—Ç—å',
    theory: '–¢–µ–æ—Ä–∏—è',
    method: '–ú–µ—Ç–æ–¥',
    algorithm: '–ê–ª–≥–æ—Ä–∏—Ç–º',
    implementation: '–ò–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è',
  },
  en: {
    field: 'Field',
    theory: 'Theory',
    method: 'Method',
    algorithm: 'Algorithm',
    implementation: 'Implementation',
  },
};

// ==================== NODES ====================

export const initialNodes: AINode[] = [
  // ========== FIELD ==========
  {
    id: 'ai',
    position: { x: 600, y: 0 },
    type: 'custom',
    data: {
      emoji: 'ü§ñ',
      level: 'field',
      ru: {
        label: '–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç',
        description: '–û–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, —Å–æ–∑–¥–∞—é—â–∞—è —Å–∏—Å—Ç–µ–º—ã, —Å–ø–æ—Å–æ–±–Ω—ã–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±—É—é—â–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.',
        keyPoints: [
          'üìÖ –¢–µ—Ä–º–∏–Ω –≤–≤–µ–¥—ë–Ω –≤ 1956 –≥–æ–¥—É (–î–∂–æ–Ω –ú–∞–∫–∫–∞—Ä—Ç–∏)',
          'üéØ –¶–µ–ª—å: –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á',
          'üîÄ –í–∫–ª—é—á–∞–µ—Ç: ML, —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫—É, NLP, CV',
          '‚ö° –°–µ–π—á–∞—Å: —ç—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ AI (2022+)',
        ],
        howItWorks: 'AI —Å–∏—Å—Ç–µ–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É: –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Üí –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∞–ª–≥–æ—Ä–∏—Ç–º—ã/–º–æ–¥–µ–ª–∏) ‚Üí –≤—ã—Ö–æ–¥–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π AI –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: –≤–º–µ—Å—Ç–æ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª, —Å–∏—Å—Ç–µ–º–∞ —É—á–∏—Ç—Å—è –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö. –ü—Ä–æ—Ü–µ—Å—Å: 1) –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö 2) –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö 3) –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ 4) –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º. AI –º–æ–∂–µ—Ç –±—ã—Ç—å —É–∑–∫–∏–º (–æ–¥–Ω–∞ –∑–∞–¥–∞—á–∞) –∏–ª–∏ –æ–±—â–∏–º (AGI ‚Äî –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á, –ø–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç).',
      },
      en: {
        label: 'Artificial Intelligence',
        description: 'Field of computer science creating systems capable of performing tasks that require human intelligence.',
        keyPoints: [
          'üìÖ Term coined in 1956 (John McCarthy)',
          'üéØ Goal: automate cognitive tasks',
          'üîÄ Includes: ML, robotics, NLP, CV',
          '‚ö° Now: generative AI era (2022+)',
        ],
        howItWorks: 'AI systems work on principle: input data ‚Üí processing (algorithms/models) ‚Üí output result. Modern AI is based on machine learning: instead of explicit rule programming, system learns from examples. Process: 1) Collect data 2) Train model on data 3) Validate quality 4) Apply to new data. AI can be narrow (single task) or general (AGI ‚Äî multiple tasks, not yet achieved).',
      },
    },
  },

  // ========== THEORY ==========
  {
    id: 'ml',
    position: { x: 150, y: 120 },
    type: 'custom',
    data: {
      emoji: 'üß†',
      level: 'theory',
      ru: {
        label: '–ú–∞—à–∏–Ω–Ω–æ–µ –û–±—É—á–µ–Ω–∏–µ',
        description: '–ü–∞—Ä–∞–¥–∏–≥–º–∞, –≥–¥–µ —Å–∏—Å—Ç–µ–º—ã —É—á–∞—Ç—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞.',
        keyPoints: [
          'üìä –î–∞–Ω–Ω—ã–µ ‚Üí –ü–∞—Ç—Ç–µ—Ä–Ω—ã ‚Üí –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è',
          'üîÑ –û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é',
          'üìà –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Ç—ë—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö',
          'üéì –¢—Ä–∏ —Ç–∏–ø–∞: supervised, unsupervised, RL',
        ],
        howItWorks: 'ML –º–æ–¥–µ–ª—å ‚Äî —ç—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–≤–µ—Å–∞–º–∏). –û–±—É—á–µ–Ω–∏–µ: 1) –ú–æ–¥–µ–ª—å –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ 2) –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º —á–µ—Ä–µ–∑ loss function 3) –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è) 4) –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ (gradient descent) 5) –ü–æ–≤—Ç–æ—Ä —Ç—ã—Å—è—á–∏ —Ä–∞–∑. –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º (inference). –ö–ª—é—á–µ–≤–æ–µ: –º–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–∞–º–∞, –∞ –Ω–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –∏—Ö –∑–∞–¥–∞—ë—Ç.',
      },
      en: {
        label: 'Machine Learning',
        description: 'Paradigm where systems learn from data without explicit programming of every rule.',
        keyPoints: [
          'üìä Data ‚Üí Patterns ‚Üí Predictions',
          'üîÑ Learning through iterative optimization',
          'üìà Quality improves with more data',
          'üéì Three types: supervised, unsupervised, RL',
        ],
        howItWorks: 'ML model is a mathematical function with adjustable parameters (weights). Training: 1) Model makes prediction 2) Compare with correct answer via loss function 3) Compute gradient (improvement direction) 4) Update weights (gradient descent) 5) Repeat thousands of times. After training, model is applied to new data (inference). Key: model finds patterns itself, programmer doesn\'t specify them.',
      },
    },
  },
  {
    id: 'dl',
    position: { x: 600, y: 120 },
    type: 'custom',
    data: {
      emoji: 'üß¨',
      level: 'theory',
      ru: {
        label: '–ì–ª—É–±–æ–∫–æ–µ –û–±—É—á–µ–Ω–∏–µ',
        description: '–ü–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ ML —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏ –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Å–ª–æ—ë–≤, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞—é—â–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏.',
        keyPoints: [
          'üèóÔ∏è –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (3+ —Å–ª–æ—ë–≤)',
          'üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤',
          'üí™ –¢—Ä–µ–±—É–µ—Ç –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö –∏ GPU',
          'üöÄ –ü—Ä–æ—Ä—ã–≤: ImageNet 2012 (AlexNet)',
        ],
        howItWorks: 'Deep Learning –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Å–ª–æ—ë–≤. –ö–∞–∂–¥—ã–π —Å–ª–æ–π –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å—ë –±–æ–ª–µ–µ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: –ø–µ—Ä–≤—ã–µ —Å–ª–æ–∏ ‚Äî –ø—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–ª–∏–Ω–∏–∏, –∫—Ä–∞—è), —Å—Ä–µ–¥–Ω–∏–µ ‚Äî —Ñ–æ—Ä–º—ã, –≥–ª—É–±–æ–∫–∏–µ ‚Äî –æ–±—ä–µ–∫—Ç—ã —Ü–µ–ª–∏–∫–æ–º. –û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ backpropagation: –æ—à–∏–±–∫–∞ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞–∑–∞–¥ –ø–æ —Å–µ—Ç–∏, –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —Å–≤–æ–∏ –≤–µ—Å–∞. "–ì–ª—É–±–∏–Ω–∞" –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏. –¢—Ä–µ–±—É–µ—Ç GPU –∏–∑-–∑–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π.',
      },
      en: {
        label: 'Deep Learning',
        description: 'Subset of ML with multi-layer neural networks that automatically extract features.',
        keyPoints: [
          'üèóÔ∏è Multi-layer neural networks (3+ layers)',
          'üîç Automatic feature extraction',
          'üí™ Requires lots of data and GPUs',
          'üöÄ Breakthrough: ImageNet 2012 (AlexNet)',
        ],
        howItWorks: 'Deep Learning uses neural networks with many layers. Each layer extracts increasingly abstract features: first layers ‚Äî simple patterns (lines, edges), middle ‚Äî shapes, deep ‚Äî whole objects. Training via backpropagation: error propagates backward through network, each neuron adjusts its weights. "Depth" allows modeling complex nonlinear dependencies. Requires GPU due to parallel matrix operations.',
      },
    },
  },
  {
    id: 'nlp',
    position: { x: 1050, y: 120 },
    type: 'custom',
    data: {
      emoji: 'üí¨',
      level: 'theory',
      ru: {
        label: '–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ø–∑—ã–∫–∞',
        description: '–¢–µ–æ—Ä–∏—è –∏ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.',
        keyPoints: [
          'üìù –¢–µ–∫—Å—Ç –∫–∞–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤',
          'üåê –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ',
          'üî§ –≠—Ç–∞–ø—ã: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Üí –º–æ–¥–µ–ª—å',
          'üó£Ô∏è –ó–∞–¥–∞—á–∏: –ø–µ—Ä–µ–≤–æ–¥, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è, QA',
        ],
        howItWorks: 'NLP –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª—å—é. –ü–∞–π–ø–ª–∞–π–Ω: 1) –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ‚Äî —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –µ–¥–∏–Ω–∏—Ü—ã (—Å–ª–æ–≤–∞/–ø–æ–¥—Å–ª–æ–≤–∞) 2) –≠–º–±–µ–¥–¥–∏–Ω–≥ ‚Äî –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã —á–∏—Å–µ–ª 3) –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–¥–µ–ª—å—é (—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä) ‚Äî –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 4) –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã—Ö–æ–¥–∞. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è: —Å–ª–æ–≤–∞ —Å –ø–æ—Ö–æ–∂–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –∏–º–µ—é—Ç –±–ª–∏–∑–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω, –≥–µ–Ω–µ—Ä–∏—Ä—É—è —Ç–µ–∫—Å—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ.',
      },
      en: {
        label: 'Natural Language Processing',
        description: 'Theory and methods for understanding, interpreting, and generating human language.',
        keyPoints: [
          'üìù Text as sequence of tokens',
          'üåê Context determines meaning',
          'üî§ Pipeline: tokenization ‚Üí embeddings ‚Üí model',
          'üó£Ô∏è Tasks: translation, summarization, QA',
        ],
        howItWorks: 'NLP converts text to numbers for model processing. Pipeline: 1) Tokenization ‚Äî split into units (words/subwords) 2) Embedding ‚Äî convert tokens to number vectors 3) Model processing (transformer) ‚Äî understand context 4) Decoding ‚Äî generate output. Key idea: words with similar meaning have close vectors. Modern LLMs predict next token, generating text autoregressively.',
      },
    },
  },

  // ========== METHOD (ML) ==========
  {
    id: 'supervised',
    position: { x: -50, y: 260 },
    type: 'custom',
    data: {
      emoji: 'üìä',
      level: 'method',
      ru: {
        label: '–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º',
        description: '–ú–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é –≤—Ö–æ–¥‚Üí–≤—ã—Ö–æ–¥.',
        keyPoints: [
          'üè∑Ô∏è –¢—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (labels)',
          'üéØ –¶–µ–ª—å: –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è',
          'üìâ Loss function –∏–∑–º–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ',
          'üîÆ –¢–∏–ø—ã: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è',
        ],
        howItWorks: '–ü—Ä–æ—Ü–µ—Å—Å: 1) –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–∞—Ä–∞–º–∏ (–≤—Ö–æ–¥, –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π_–æ—Ç–≤–µ—Ç) 2) –ú–æ–¥–µ–ª—å –¥–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –≤—Ö–æ–¥–∞ 3) Loss function –≤—ã—á–∏—Å–ª—è–µ—Ç –æ—à–∏–±–∫—É –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º 4) –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ —á—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å –æ—à–∏–±–∫—É 5) –ü–æ–≤—Ç–æ—Ä –Ω–∞ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –º–Ω–æ–≥–æ —ç–ø–æ—Ö. –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –≤—ã—Ö–æ–¥ ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤, loss ‚Äî cross-entropy. –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏: –≤—ã—Ö–æ–¥ ‚Äî —á–∏—Å–ª–æ, loss ‚Äî MSE.',
      },
      en: {
        label: 'Supervised Learning',
        description: 'Learning method using labeled data: model learns input‚Üíoutput mapping.',
        keyPoints: [
          'üè∑Ô∏è Requires labeled data',
          'üéØ Goal: minimize prediction error',
          'üìâ Loss function measures quality',
          'üîÆ Types: classification, regression',
        ],
        howItWorks: 'Process: 1) Prepare dataset with (input, correct_answer) pairs 2) Model makes prediction for input 3) Loss function computes error between prediction and correct answer 4) Optimizer updates weights to reduce error 5) Repeat on all examples for many epochs. For classification: output ‚Äî class probabilities, loss ‚Äî cross-entropy. For regression: output ‚Äî number, loss ‚Äî MSE.',
      },
    },
  },
  {
    id: 'unsupervised',
    position: { x: 150, y: 260 },
    type: 'custom',
    data: {
      emoji: 'üîç',
      level: 'method',
      ru: {
        label: '–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è',
        description: '–ú–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.',
        keyPoints: [
          '‚ùå –ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö',
          'üîé –ù–∞—Ö–æ–¥–∏—Ç —Å–∫—Ä—ã—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É',
          'üìä –¢–∏–ø—ã: –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏',
          'üí° –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è, –∞–Ω–æ–º–∞–ª–∏–∏',
        ],
        howItWorks: '–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –±–µ–∑ "–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤". –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –æ–±—ä–µ–∫—Ç—ã –≤–º–µ—Å—Ç–µ (K-Means –Ω–∞—Ö–æ–¥–∏—Ç —Ü–µ–Ω—Ç—Ä—ã –≥—Ä—É–ø–ø). –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: –Ω–∞—Ö–æ–¥–∏—Ç –≥–ª–∞–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∞—Ä–∏–∞—Ü–∏–∏ (PCA –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç –Ω–∞ –º–µ–Ω—å—à–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ). –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π: —É—á–∏—Ç—Å—è –Ω–∞ "–Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö" –¥–∞–Ω–Ω—ã—Ö, –≤—ã–¥–µ–ª—è–µ—Ç –Ω–µ–æ–±—ã—á–Ω—ã–µ. –ö–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ: –Ω–µ—Ç —è–≤–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, –º–æ–¥–µ–ª—å —Å–∞–º–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É.',
      },
      en: {
        label: 'Unsupervised Learning',
        description: 'Method for finding hidden patterns in unlabeled data.',
        keyPoints: [
          '‚ùå No labeled data required',
          'üîé Finds hidden structure',
          'üìä Types: clustering, dimensionality reduction',
          'üí° Uses: segmentation, anomaly detection',
        ],
        howItWorks: 'Model analyzes data without "correct answers". Clustering: groups similar objects together (K-Means finds group centers). Dimensionality reduction: finds main variation directions (PCA projects to smaller space). Anomaly detection: learns from "normal" data, identifies unusual ones. Key difference: no explicit target variable, model determines structure itself.',
      },
    },
  },
  {
    id: 'rl',
    position: { x: 350, y: 260 },
    type: 'custom',
    data: {
      emoji: 'üéÆ',
      level: 'method',
      ru: {
        label: '–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º',
        description: '–ú–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ä–µ–¥–æ–π –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥/—à—Ç—Ä–∞—Ñ–æ–≤.',
        keyPoints: [
          'üéØ –ê–≥–µ–Ω—Ç ‚Üí –î–µ–π—Å—Ç–≤–∏–µ ‚Üí –°—Ä–µ–¥–∞ ‚Üí –ù–∞–≥—Ä–∞–¥–∞',
          '‚öñÔ∏è –ë–∞–ª–∞–Ω—Å: –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ vs –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ',
          'üèÜ –¶–µ–ª—å: –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è —Å—É–º–º–∞—Ä–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã',
          'üé≤ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: –∏–≥—Ä—ã, —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞, —Ç–æ—Ä–≥–æ–≤–ª—è',
        ],
        howItWorks: '–ê–≥–µ–Ω—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ (state), –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ (action), –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –≤ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞–≥—Ä–∞–¥—É (reward). –¶–µ–ª—å: –≤—ã—É—á–∏—Ç—å policy ‚Äî —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏–π –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—â—É—é —Å—É–º–º–∞—Ä–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É. –î–∏–ª–µ–º–º–∞ exploration/exploitation: –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ö–æ—Ä–æ—à–∏–µ? –ú–µ—Ç–æ–¥—ã: Q-learning (—Ç–∞–±–ª–∏—Ü–∞ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π), Policy Gradient (–Ω–∞–ø—Ä—è–º—É—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é), Actor-Critic (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è).',
      },
      en: {
        label: 'Reinforcement Learning',
        description: 'Method where agent learns through interaction with environment and rewards/penalties.',
        keyPoints: [
          'üéØ Agent ‚Üí Action ‚Üí Environment ‚Üí Reward',
          '‚öñÔ∏è Balance: exploration vs exploitation',
          'üèÜ Goal: maximize cumulative reward',
          'üé≤ Uses: games, robotics, trading',
        ],
        howItWorks: 'Agent is in state, chooses action, transitions to new state and receives reward. Goal: learn policy ‚Äî action selection strategy maximizing cumulative reward. Exploration/exploitation dilemma: explore new actions or use known good ones? Methods: Q-learning (action value table), Policy Gradient (directly optimize strategy), Actor-Critic (combination).',
      },
    },
  },

  // ========== ALGORITHM (ML) ==========
  {
    id: 'linear-reg',
    position: { x: -150, y: 400 },
    type: 'custom',
    data: {
      emoji: 'üìà',
      level: 'algorithm',
      ru: {
        label: '–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è',
        description: '–ü—Ä–æ—Å—Ç–µ–π—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å.',
        keyPoints: [
          'üìê –§–æ—Ä–º—É–ª–∞: y = wx + b',
          'üéØ –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è MSE (Mean Squared Error)',
          '‚ö° –ë—ã—Å—Ç—Ä—ã–π, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π',
          'üìä –ë–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏',
        ],
        howItWorks: '–ú–æ–¥–µ–ª—å: y = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + b, –≥–¥–µ w ‚Äî –≤–µ—Å–∞, b ‚Äî —Å–º–µ—â–µ–Ω–∏–µ. –û–±—É—á–µ–Ω–∏–µ: 1) –ù–∞—á–∞—Ç—å —Å —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ—Å–æ–≤ 2) –í—ã—á–∏—Å–ª–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è 3) –ü–æ—Å—á–∏—Ç–∞—Ç—å MSE = —Å—Ä–µ–¥–Ω–µ–µ((–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - —Ñ–∞–∫—Ç)¬≤) 4) –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å –≤–µ—Å–∞ 5) –û–±–Ω–æ–≤–∏—Ç—å: w = w - learning_rate * –≥—Ä–∞–¥–∏–µ–Ω—Ç. –ï—Å—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –ª–∏–Ω–µ–π–Ω—É—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å ‚Äî –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.',
      },
      en: {
        label: 'Linear Regression',
        description: 'Simplest algorithm for predicting continuous values through linear relationship.',
        keyPoints: [
          'üìê Formula: y = wx + b',
          'üéØ Minimizes MSE (Mean Squared Error)',
          '‚ö° Fast, interpretable',
          'üìä Baseline algorithm for regression',
        ],
        howItWorks: 'Model: y = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + b, where w ‚Äî weights, b ‚Äî bias. Training: 1) Start with random weights 2) Compute predictions 3) Calculate MSE = mean((prediction - actual)¬≤) 4) Gradient shows how to change weights 5) Update: w = w - learning_rate * gradient. Has analytical solution via normal equation. Assumes linear relationship ‚Äî not suitable for complex patterns.',
      },
    },
  },
  {
    id: 'decision-tree',
    position: { x: 30, y: 400 },
    type: 'custom',
    data: {
      emoji: 'üå≥',
      level: 'algorithm',
      ru: {
        label: '–î–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π',
        description: '–ê–ª–≥–æ—Ä–∏—Ç–º, —Ä–∞–∑–±–∏–≤–∞—é—â–∏–π –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É—Å–ª–æ–≤–∏–π (if-else).',
        keyPoints: [
          'üîÄ –ö–∞–∂–¥—ã–π —É–∑–µ–ª = —É—Å–ª–æ–≤–∏–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è',
          'üçÉ –õ–∏—Å—Ç—å—è = –∏—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ',
          'üëÅÔ∏è –õ–µ–≥–∫–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –ø–æ–Ω—è—Ç—å',
          '‚ö†Ô∏è –°–∫–ª–æ–Ω–µ–Ω –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é',
        ],
        howItWorks: '–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ: 1) –í—ã–±—Ä–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫ –∏ –ø–æ—Ä–æ–≥ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è 2) –ö—Ä–∏—Ç–µ—Ä–∏–π: –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è "—á–∏—Å—Ç–æ—Ç—ã" –≥—Ä—É–ø–ø (Gini, Entropy) 3) –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞–∑–±–∏–≤–∞—Ç—å –¥–æ —É—Å–ª–æ–≤–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏. –ü—Ä–∏–º–µ—Ä: "–í–æ–∑—Ä–∞—Å—Ç > 30? –î–∞ ‚Üí –î–æ—Ö–æ–¥ > 50k? –î–∞ ‚Üí –û–¥–æ–±—Ä–∏—Ç—å –∫—Ä–µ–¥–∏—Ç". –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: –∏–¥—ë–º –æ—Ç –∫–æ—Ä–Ω—è –ø–æ —É—Å–ª–æ–≤–∏—è–º –¥–æ –ª–∏—Å—Ç–∞. –õ–µ–≥–∫–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å, –Ω–æ –≥–ª—É–±–æ–∫–æ–µ –¥–µ—Ä–µ–≤–æ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ (overfitting). –†–µ—à–µ–Ω–∏–µ: –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –≥–ª—É–±–∏–Ω—É –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω—Å–∞–º–±–ª–∏.',
      },
      en: {
        label: 'Decision Tree',
        description: 'Algorithm splitting data through sequence of conditions (if-else).',
        keyPoints: [
          'üîÄ Each node = split condition',
          'üçÉ Leaves = final prediction',
          'üëÅÔ∏è Easy to visualize and understand',
          '‚ö†Ô∏è Prone to overfitting',
        ],
        howItWorks: 'Building: 1) Choose feature and threshold for split 2) Criterion: maximize group "purity" (Gini, Entropy) 3) Recursively split until stopping condition. Example: "Age > 30? Yes ‚Üí Income > 50k? Yes ‚Üí Approve loan". Prediction: go from root through conditions to leaf. Easy to interpret, but deep tree memorizes training data (overfitting). Solution: limit depth or use ensembles.',
      },
    },
  },
  {
    id: 'random-forest',
    position: { x: 210, y: 400 },
    type: 'custom',
    data: {
      emoji: 'üå≤',
      level: 'algorithm',
      ru: {
        label: 'Random Forest',
        description: '–ê–Ω—Å–∞–º–±–ª—å –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π, –≥–æ–ª–æ—Å—É—é—â–∏—Ö –∑–∞ –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.',
        keyPoints: [
          'üå≥ –ú–Ω–æ–≥–æ –¥–µ—Ä–µ–≤—å–µ–≤ (100-1000)',
          'üé≤ –ö–∞–∂–¥–æ–µ –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ',
          'üó≥Ô∏è –ì–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ/—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤',
          'üí™ –£—Å—Ç–æ–π—á–∏–≤ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é',
        ],
        howItWorks: '–ò–¥–µ—è: –º–Ω–æ–≥–æ "—Å–ª–∞–±—ã—Ö" –º–æ–¥–µ–ª–µ–π –≤–º–µ—Å—Ç–µ –¥–∞—é—Ç "—Å–∏–ª—å–Ω—É—é". –û–±—É—á–µ–Ω–∏–µ: 1) –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ –≤–∑—è—Ç—å bootstrap-–≤—ã–±–æ—Ä–∫—É (—Å –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è–º–∏) 2) –í –∫–∞–∂–¥–æ–º —É–∑–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ 3) –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–µ—Ä–µ–≤–æ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è ‚Äî –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞, —Ä–µ–≥—Ä–µ—Å—Å–∏—è ‚Äî —Å—Ä–µ–¥–Ω–µ–µ. –†–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏—è —É–º–µ–Ω—å—à–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –¥–µ—Ä–µ–≤—å—è–º–∏ ‚Üí —Å–Ω–∏–∂–∞–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é ‚Üí –º–µ–Ω—å—à–µ overfitting. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫: —Ç–µ—Ä—è–µ—Ç—Å—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å.',
      },
      en: {
        label: 'Random Forest',
        description: 'Ensemble of decision trees voting for final result.',
        keyPoints: [
          'üå≥ Many trees (100-1000)',
          'üé≤ Each on random sample',
          'üó≥Ô∏è Voting/averaging results',
          'üí™ Robust to overfitting',
        ],
        howItWorks: 'Idea: many "weak" models together make "strong" one. Training: 1) For each tree take bootstrap sample (with replacement) 2) At each node consider random subset of features 3) Build tree without restrictions. Prediction: classification ‚Äî majority vote, regression ‚Äî average. Randomization reduces correlation between trees ‚Üí reduces variance ‚Üí less overfitting. Downside: loses interpretability.',
      },
    },
  },
  {
    id: 'svm',
    position: { x: -150, y: 520 },
    type: 'custom',
    data: {
      emoji: '‚öîÔ∏è',
      level: 'algorithm',
      ru: {
        label: 'SVM',
        description: 'Support Vector Machine ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤.',
        keyPoints: [
          'üìè –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ—Ç—Å—Ç—É–ø –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏',
          'üîÆ Kernel trick –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –≥—Ä–∞–Ω–∏—Ü',
          'üí™ –†–∞–±–æ—Ç–∞–µ—Ç –≤ –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö',
          'üìä –•–æ—Ä–æ—à –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤',
        ],
        howItWorks: '–¶–µ–ª—å: –Ω–∞–π—Ç–∏ –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–¥–∞–ª—ë–Ω–Ω—É—é –æ—Ç –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ—á–µ–∫ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤ (support vectors). Margin = —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ—á–µ–∫ √ó 2. Kernel trick: –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –±–æ–ª—å—à–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≥–¥–µ –æ–Ω–∏ –ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã. –ü—Ä–∏–º–µ—Ä—ã —è–¥–µ—Ä: RBF (—Ä–∞–¥–∏–∞–ª—å–Ω–æ–µ), polynomial. Soft margin: –¥–æ–ø—É—Å–∫–∞–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–∏ –Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏: —Ä–µ—à–∞–µ—Ç—Å—è –∫–∞–∫ –∑–∞–¥–∞—á–∞ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.',
      },
      en: {
        label: 'SVM',
        description: 'Support Vector Machine ‚Äî finds optimal hyperplane to separate classes.',
        keyPoints: [
          'üìè Maximizes margin between classes',
          'üîÆ Kernel trick for nonlinear boundaries',
          'üí™ Works in high dimensions',
          'üìä Good for small datasets',
        ],
        howItWorks: 'Goal: find hyperplane maximally distant from nearest points of both classes (support vectors). Margin = distance to nearest points √ó 2. Kernel trick: projects data to higher-dimensional space where linearly separable. Kernel examples: RBF (radial), polynomial. Soft margin: allows errors for non-separable data. Mathematically: solved as quadratic programming problem.',
      },
    },
  },
  {
    id: 'kmeans',
    position: { x: 30, y: 520 },
    type: 'custom',
    data: {
      emoji: 'üéØ',
      level: 'algorithm',
      ru: {
        label: 'K-Means',
        description: '–ê–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –≥—Ä—É–ø–ø–∏—Ä—É—é—â–∏–π –¥–∞–Ω–Ω—ã–µ –≤ K –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ –±–ª–∏–∑–æ—Å—Ç–∏ –∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º.',
        keyPoints: [
          'üî¢ K ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∑–∞–¥–∞—ë—Ç—Å—è)',
          'üîÑ –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ: –Ω–∞–∑–Ω–∞—á–∏—Ç—å ‚Üí –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å —Ü–µ–Ω—Ç—Ä—ã',
          'üìè –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ',
          '‚ö° –ë—ã—Å—Ç—Ä—ã–π, –Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –Ω–∞—á–∞–ª—å–Ω—ã–º —Ç–æ—á–∫–∞–º',
        ],
        howItWorks: '–ê–ª–≥–æ—Ä–∏—Ç–º: 1) –°–ª—É—á–∞–π–Ω–æ –≤—ã–±—Ä–∞—Ç—å K –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω—Ç—Ä–æ–≤ 2) –ö–∞–∂–¥—É—é —Ç–æ—á–∫—É –æ—Ç–Ω–µ—Å—Ç–∏ –∫ –±–ª–∏–∂–∞–π—à–µ–º—É —Ü–µ–Ω—Ç—Ä—É 3) –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å —Ü–µ–Ω—Ç—Ä—ã –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ —Ç–æ—á–µ–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞ 4) –ü–æ–≤—Ç–æ—Ä—è—Ç—å 2-3 –ø–æ–∫–∞ —Ü–µ–Ω—Ç—Ä—ã –Ω–µ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—Ç—Å—è. –ö—Ä–∏—Ç–µ—Ä–∏–π: –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —Å—É–º–º—ã –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ —Ü–µ–Ω—Ç—Ä–æ–≤ (inertia). –ü—Ä–æ–±–ª–µ–º—ã: –Ω—É–∂–Ω–æ –∑–∞—Ä–∞–Ω–µ–µ –∑–Ω–∞—Ç—å K (Elbow method –ø–æ–º–æ–≥–∞–µ—Ç), —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω—Ç—Ä–æ–≤ (K-means++ —É–ª—É—á—à–∞–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é).',
      },
      en: {
        label: 'K-Means',
        description: 'Clustering algorithm grouping data into K clusters by proximity to centroids.',
        keyPoints: [
          'üî¢ K ‚Äî number of clusters (specified)',
          'üîÑ Iterative: assign ‚Üí recalculate centers',
          'üìè Minimizes within-cluster distance',
          '‚ö° Fast, but sensitive to initial points',
        ],
        howItWorks: 'Algorithm: 1) Randomly choose K initial centers 2) Assign each point to nearest center 3) Recalculate centers as mean of cluster points 4) Repeat 2-3 until centers stabilize. Criterion: minimize sum of squared distances to centers (inertia). Problems: need to know K beforehand (Elbow method helps), result depends on initial centers (K-means++ improves initialization).',
      },
    },
  },
  {
    id: 'pca',
    position: { x: 210, y: 520 },
    type: 'custom',
    data: {
      emoji: 'üìâ',
      level: 'algorithm',
      ru: {
        label: 'PCA',
        description: 'Principal Component Analysis ‚Äî —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–∞–∫—Å–∏–º—É–º–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.',
        keyPoints: [
          'üìä –ù–∞—Ö–æ–¥–∏—Ç –≥–ª–∞–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∞—Ä–∏–∞—Ü–∏–∏',
          'üîΩ –ü—Ä–æ–µ–∫—Ü–∏—è –Ω–∞ –º–µ–Ω—å—à–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ',
          'üìà –°–æ—Ö—Ä–∞–Ω—è–µ—Ç % –æ–±—ä—è—Å–Ω—ë–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏',
          'üëÅÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏',
        ],
        howItWorks: '–ò–¥–µ—è: –Ω–∞–π—Ç–∏ –Ω–æ–≤—ã–µ –æ—Å–∏ (–≥–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã) –ø–æ –∫–æ—Ç–æ—Ä—ã–º –¥–∞–Ω–Ω—ã–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ "—Ä–∞—Å—Ç—è–Ω—É—Ç—ã". –ê–ª–≥–æ—Ä–∏—Ç–º: 1) –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (–≤—ã—á–µ—Å—Ç—å —Å—Ä–µ–¥–Ω–µ–µ) 2) –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É 3) –ù–∞–π—Ç–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –∏ –∑–Ω–∞—á–µ–Ω–∏—è 4) –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π 5) –í—ã–±—Ä–∞—Ç—å —Ç–æ–ø-K –∫–æ–º–ø–æ–Ω–µ–Ω—Ç 6) –ü—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü–µ—Ä–≤–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –æ–±—ä—è—Å–Ω—è–µ—Ç –º–∞–∫—Å–∏–º—É–º –¥–∏—Å–ø–µ—Ä—Å–∏–∏, –≤—Ç–æ—Ä–∞—è ‚Äî –º–∞–∫—Å–∏–º—É–º –æ—Å—Ç–∞–≤—à–µ–π—Å—è, –∏ —Ç.–¥.',
      },
      en: {
        label: 'PCA',
        description: 'Principal Component Analysis ‚Äî dimensionality reduction preserving maximum information.',
        keyPoints: [
          'üìä Finds main directions of variation',
          'üîΩ Projects to lower dimensions',
          'üìà Preserves % of explained variance',
          'üëÅÔ∏è Used for visualization',
        ],
        howItWorks: 'Idea: find new axes (principal components) along which data is maximally "stretched". Algorithm: 1) Center data (subtract mean) 2) Compute covariance matrix 3) Find eigenvectors and eigenvalues 4) Sort by decreasing eigenvalues 5) Select top-K components 6) Project data. First component explains maximum variance, second ‚Äî maximum of remaining, etc.',
      },
    },
  },
  {
    id: 'qlearning',
    position: { x: 390, y: 400 },
    type: 'custom',
    data: {
      emoji: 'üé∞',
      level: 'algorithm',
      ru: {
        label: 'Q-Learning',
        description: '–ê–ª–≥–æ—Ä–∏—Ç–º RL, –æ–±—É—á–∞—é—â–∏–π —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –±–µ–∑ –º–æ–¥–µ–ª–∏ —Å—Ä–µ–¥—ã.',
        keyPoints: [
          'üìä Q(s,a) = –æ–∂–∏–¥–∞–µ–º–∞—è –Ω–∞–≥—Ä–∞–¥–∞',
          'üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞',
          'üé≤ Œµ-greedy –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è',
          'üö´ Model-free: –Ω–µ –∑–Ω–∞–µ—Ç –ø—Ä–∞–≤–∏–ª —Å—Ä–µ–¥—ã',
        ],
        howItWorks: 'Q-—Ç–∞–±–ª–∏—Ü–∞ —Ö—Ä–∞–Ω–∏—Ç —Ü–µ–Ω–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –≤ –∫–∞–∂–¥–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s\',a\')) - Q(s,a)], –≥–¥–µ Œ± ‚Äî learning rate, Œ≥ ‚Äî discount factor, r ‚Äî –Ω–∞–≥—Ä–∞–¥–∞, s\' ‚Äî –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ. Œµ-greedy: —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é Œµ –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (exploration), –∏–Ω–∞—á–µ –ª—É—á—à–µ–µ –ø–æ Q (exploitation). Deep Q-Network (DQN): –∑–∞–º–µ–Ω—è–µ—Ç —Ç–∞–±–ª–∏—Ü—É –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ —Å–æ—Å—Ç–æ—è–Ω–∏–π.',
      },
      en: {
        label: 'Q-Learning',
        description: 'RL algorithm learning action-value function without environment model.',
        keyPoints: [
          'üìä Q(s,a) = expected reward',
          'üîÑ Updates via Bellman equation',
          'üé≤ Œµ-greedy for exploration',
          'üö´ Model-free: no environment rules',
        ],
        howItWorks: 'Q-table stores value of each action in each state. Update: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s\',a\')) - Q(s,a)], where Œ± ‚Äî learning rate, Œ≥ ‚Äî discount factor, r ‚Äî reward, s\' ‚Äî new state. Œµ-greedy: with probability Œµ choose random action (exploration), otherwise best by Q (exploitation). Deep Q-Network (DQN): replaces table with neural network for large state spaces.',
      },
    },
  },

  // ========== ALGORITHM (DL architectures) ==========
  {
    id: 'nn',
    position: { x: 520, y: 260 },
    type: 'custom',
    data: {
      emoji: 'üîÆ',
      level: 'algorithm',
      ru: {
        label: '–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏',
        description: '–ê–ª–≥–æ—Ä–∏—Ç–º –∏–∑ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤, –ø–µ—Ä–µ–¥–∞—é—â–∏—Ö –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª—ã.',
        keyPoints: [
          'üß† –í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω—ã –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –º–æ–∑–≥–æ–º',
          '‚ö° –ù–µ–π—Ä–æ–Ω: –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ + –∞–∫—Ç–∏–≤–∞—Ü–∏—è',
          'üîÑ –û–±—É—á–µ–Ω–∏–µ: backpropagation + gradient descent',
          'üèóÔ∏è –°–ª–æ–∏: input ‚Üí hidden ‚Üí output',
        ],
        howItWorks: '–ù–µ–π—Ä–æ–Ω –≤—ã—á–∏—Å–ª—è–µ—Ç: output = activation(Œ£(w·µ¢¬∑x·µ¢) + b). –ê–∫—Ç–∏–≤–∞—Ü–∏–∏: ReLU (max(0,x)), Sigmoid (0-1), Tanh (-1 to 1). Forward pass: –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —Å–ª–æ–π –∑–∞ —Å–ª–æ–µ–º. Backward pass (backprop): –≤—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç loss –ø–æ –∫–∞–∂–¥–æ–º—É –≤–µ—Å—É —á–µ—Ä–µ–∑ chain rule (–ø—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–æ—á–∫–∏). –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤: w = w - lr¬∑‚àÇL/‚àÇw. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤, –Ω–µ–π—Ä–æ–Ω–æ–≤, learning rate, batch size. Universal Approximation: –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≥–ª—É–±–æ–∫–∞—è —Å–µ—Ç—å –º–æ–∂–µ—Ç –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—É—é —Ñ—É–Ω–∫—Ü–∏—é.',
      },
      en: {
        label: 'Neural Networks',
        description: 'Algorithm of connected artificial neurons that transmit and transform signals.',
        keyPoints: [
          'üß† Inspired by biological brain',
          '‚ö° Neuron: weighted sum + activation',
          'üîÑ Training: backpropagation + gradient descent',
          'üèóÔ∏è Layers: input ‚Üí hidden ‚Üí output',
        ],
        howItWorks: 'Neuron computes: output = activation(Œ£(w·µ¢¬∑x·µ¢) + b). Activations: ReLU (max(0,x)), Sigmoid (0-1), Tanh (-1 to 1). Forward pass: data flows layer by layer. Backward pass (backprop): compute loss gradient for each weight via chain rule. Weight update: w = w - lr¬∑‚àÇL/‚àÇw. Hyperparameters: number of layers, neurons, learning rate, batch size. Universal Approximation: sufficiently deep network can approximate any function.',
      },
    },
  },
  {
    id: 'cnn',
    position: { x: 700, y: 260 },
    type: 'custom',
    data: {
      emoji: 'üëÅÔ∏è',
      level: 'algorithm',
      ru: {
        label: 'CNN',
        description: '–°–≤—ë—Ä—Ç–æ—á–Ω—ã–µ —Å–µ—Ç–∏ ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å —Å–µ—Ç–æ—á–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è).',
        keyPoints: [
          'üî≤ –°–≤—ë—Ä—Ç–∫–∞: —Å–∫–æ–ª—å–∑—è—â–∏–π —Ñ–∏–ª—å—Ç—Ä –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é',
          'üìê Pooling: —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏',
          'üé® –ò–µ—Ä–∞—Ä—Ö–∏—è: –∫—Ä–∞—è ‚Üí —Ñ–æ—Ä–º—ã ‚Üí –æ–±—ä–µ–∫—Ç—ã',
          'üì∏ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, –¥–µ—Ç–µ–∫—Ü–∏—è',
        ],
        howItWorks: '–°–≤—ë—Ä—Ç–∫–∞: —Ñ–∏–ª—å—Ç—Ä (—è–¥—Ä–æ) 3x3 –∏–ª–∏ 5x5 —Å–∫–æ–ª—å–∑–∏—Ç –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –≤—ã—á–∏—Å–ª—è—è —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ. –û–¥–∏–Ω —Ñ–∏–ª—å—Ç—Ä = –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫ (–≥—Ä–∞–Ω—å, —Ü–≤–µ—Ç). Stride ‚Äî —à–∞–≥, padding ‚Äî –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü. Pooling (–æ–±—ã—á–Ω–æ max): —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä, —Å–æ—Ö—Ä–∞–Ω—è—è –≥–ª–∞–≤–Ω–æ–µ. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: [Conv‚ÜíReLU‚ÜíPool] √ó N ‚Üí Flatten ‚Üí Dense. –ü—Ä–∏–º–µ—Ä—ã: LeNet (1998), AlexNet (2012), VGG, ResNet (skip connections –¥–ª—è –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π).',
      },
      en: {
        label: 'CNN',
        description: 'Convolutional networks ‚Äî algorithm for processing grid-structured data (images).',
        keyPoints: [
          'üî≤ Convolution: sliding filter over image',
          'üìê Pooling: dimensionality reduction',
          'üé® Hierarchy: edges ‚Üí shapes ‚Üí objects',
          'üì∏ Uses: recognition, detection',
        ],
        howItWorks: 'Convolution: filter (kernel) 3x3 or 5x5 slides over image, computing dot product. One filter = one feature (edge, color). Stride ‚Äî step, padding ‚Äî adding borders. Pooling (usually max): reduces size, preserving main info. Architecture: [Conv‚ÜíReLU‚ÜíPool] √ó N ‚Üí Flatten ‚Üí Dense. Examples: LeNet (1998), AlexNet (2012), VGG, ResNet (skip connections for very deep networks).',
      },
    },
  },
  {
    id: 'rnn',
    position: { x: 520, y: 400 },
    type: 'custom',
    data: {
      emoji: 'üîÅ',
      level: 'algorithm',
      ru: {
        label: 'RNN',
        description: '–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏ ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å–æ—Ö—Ä–∞–Ω—è—è "–ø–∞–º—è—Ç—å" –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–∞—Ö.',
        keyPoints: [
          'üîÑ –û–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Å–ª–æ–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ',
          'üí≠ Hidden state —Ö—Ä–∞–Ω–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—à–ª–æ–º',
          'üìù –î–ª—è —Ç–µ–∫—Å—Ç–∞, –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –º—É–∑—ã–∫–∏',
          '‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞: vanishing gradient –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö',
        ],
        howItWorks: '–ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ t: h‚Çú = tanh(W‚Çï‚Çï¬∑h‚Çú‚Çã‚ÇÅ + W‚Çì‚Çï¬∑x‚Çú), –≥–¥–µ h ‚Äî hidden state, x ‚Äî –≤—Ö–æ–¥. –í–µ—Å–∞ W –æ–±—â–∏–µ –¥–ª—è –≤—Å–µ—Ö —à–∞–≥–æ–≤ (weight sharing). –ü—Ä–æ–±–ª–µ–º–∞ vanishing gradient: –ø—Ä–∏ backprop —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ —à–∞–≥–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —É–º–µ–Ω—å—à–∞—é—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ ‚Üí —Å–µ—Ç—å –Ω–µ —É—á–∏—Ç—Å—è –Ω–∞ –¥–∞–ª—å–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö. –†–µ—à–µ–Ω–∏—è: LSTM –∏ GRU –¥–æ–±–∞–≤–ª—è—é—Ç "–≤–æ—Ä–æ—Ç–∞" –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–æ—Ç–æ–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. Bidirectional RNN: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –æ–±–µ —Å—Ç–æ—Ä–æ–Ω—ã.',
      },
      en: {
        label: 'RNN',
        description: 'Recurrent networks ‚Äî process sequences, maintaining "memory" of previous steps.',
        keyPoints: [
          'üîÑ Same layer applied at each step',
          'üí≠ Hidden state stores past information',
          'üìù For text, time series, music',
          '‚ö†Ô∏è Problem: vanishing gradient on long sequences',
        ],
        howItWorks: 'At each step t: h‚Çú = tanh(W‚Çï‚Çï¬∑h‚Çú‚Çã‚ÇÅ + W‚Çì‚Çï¬∑x‚Çú), where h ‚Äî hidden state, x ‚Äî input. Weights W shared across all steps (weight sharing). Vanishing gradient problem: during backprop through many steps gradients decrease exponentially ‚Üí network doesn\'t learn long dependencies. Solutions: LSTM and GRU add "gates" to control information flow. Bidirectional RNN: processes sequence in both directions.',
      },
    },
  },
  {
    id: 'lstm',
    position: { x: 520, y: 520 },
    type: 'custom',
    data: {
      emoji: 'üß†',
      level: 'algorithm',
      ru: {
        label: 'LSTM',
        description: 'Long Short-Term Memory ‚Äî RNN —Å –≤–æ—Ä–æ—Ç–∞–º–∏, —Ä–µ—à–∞—é—â–∞—è –ø—Ä–æ–±–ª–µ–º—É –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.',
        keyPoints: [
          'üö™ 3 –≤–æ—Ä–æ—Ç: forget, input, output',
          'üì¶ Cell state ‚Äî –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å',
          'üîó –ú–æ–∂–µ—Ç –ø–æ–º–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ —Å–æ—Ç–Ω–∏ —à–∞–≥–æ–≤',
          'üìà –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è seq2seq –¥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤',
        ],
        howItWorks: '–¢—Ä–∏ –≤–æ—Ä–æ—Ç (0-1 —á–µ—Ä–µ–∑ sigmoid): Forget gate: f‚Çú = œÉ(Wf¬∑[h‚Çú‚Çã‚ÇÅ,x‚Çú]) ‚Äî —á—Ç–æ –∑–∞–±—ã—Ç—å –∏–∑ cell state. Input gate: i‚Çú = œÉ(Wi¬∑[h‚Çú‚Çã‚ÇÅ,x‚Çú]) ‚Äî —á—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤–æ–≥–æ. Output gate: o‚Çú = œÉ(Wo¬∑[h‚Çú‚Çã‚ÇÅ,x‚Çú]) ‚Äî —á—Ç–æ –≤—ã–≤–µ—Å—Ç–∏. Cell state –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è: C‚Çú = f‚Çú‚äôC‚Çú‚Çã‚ÇÅ + i‚Çú‚äôtanh(Wc¬∑[h‚Çú‚Çã‚ÇÅ,x‚Çú]). –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø—Ä–æ—Ç–µ–∫–∞—é—Ç —á–µ—Ä–µ–∑ cell state –±–µ–∑ –∑–∞—Ç—É—Ö–∞–Ω–∏—è. GRU ‚Äî —É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å 2 –≤–æ—Ä–æ—Ç–∞–º–∏.',
      },
      en: {
        label: 'LSTM',
        description: 'Long Short-Term Memory ‚Äî RNN with gates solving vanishing gradient problem.',
        keyPoints: [
          'üö™ 3 gates: forget, input, output',
          'üì¶ Cell state ‚Äî long-term memory',
          'üîó Can remember information for hundreds of steps',
          'üìà Standard for seq2seq before transformers',
        ],
        howItWorks: 'Three gates (0-1 via sigmoid): Forget gate: f‚Çú = œÉ(Wf¬∑[h‚Çú‚Çã‚ÇÅ,x‚Çú]) ‚Äî what to forget from cell state. Input gate: i‚Çú = œÉ(Wi¬∑[h‚Çú‚Çã‚ÇÅ,x‚Çú]) ‚Äî what new to add. Output gate: o‚Çú = œÉ(Wo¬∑[h‚Çú‚Çã‚ÇÅ,x‚Çú]) ‚Äî what to output. Cell state updates: C‚Çú = f‚Çú‚äôC‚Çú‚Çã‚ÇÅ + i‚Çú‚äôtanh(Wc¬∑[h‚Çú‚Çã‚ÇÅ,x‚Çú]). Gradients flow through cell state without vanishing. GRU ‚Äî simplified version with 2 gates.',
      },
    },
  },
  {
    id: 'transformer',
    position: { x: 700, y: 400 },
    type: 'custom',
    data: {
      emoji: '‚ö°',
      level: 'algorithm',
      ru: {
        label: 'Transformer',
        description: '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—â–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.',
        keyPoints: [
          'üëÄ Self-attention: –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω "–≤–∏–¥–∏—Ç" –≤—Å–µ –¥—Ä—É–≥–∏–µ',
          '‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (vs RNN –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)',
          'üìç Positional encoding –¥–ª—è –ø–æ—Ä—è–¥–∫–∞',
          'üìÑ –°—Ç–∞—Ç—å—è "Attention Is All You Need" (2017)',
        ],
        howItWorks: 'Self-attention: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤—ã—á–∏—Å–ª—è–µ–º Query, Key, Value —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏. Attention(Q,K,V) = softmax(QK·µÄ/‚àöd)V. –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –ø–æ–ª—É—á–∞–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—É—é —Å—É–º–º—É –≤—Å–µ—Ö Value, –≤–µ—Å–∞ = —Å—Ö–æ–¥—Å—Ç–≤–æ Query –∏ Key. Multi-head: –Ω–µ—Å–∫–æ–ª—å–∫–æ attention –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ ‚Üí —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —Å–≤—è–∑–µ–π. Encoder: self-attention + feed-forward. Decoder: masked self-attention (–≤–∏–¥–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ—à–ª–æ–µ) + cross-attention –∫ encoder. Positional encoding –¥–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏.',
      },
      en: {
        label: 'Transformer',
        description: 'Architecture based on attention mechanism, processing sequences in parallel.',
        keyPoints: [
          'üëÄ Self-attention: each token "sees" all others',
          '‚ö° Parallel processing (vs RNN sequential)',
          'üìç Positional encoding for order',
          'üìÑ Paper "Attention Is All You Need" (2017)',
        ],
        howItWorks: 'Self-attention: for each token compute Query, Key, Value via linear projections. Attention(Q,K,V) = softmax(QK·µÄ/‚àöd)V. Each token gets weighted sum of all Values, weights = Query-Key similarity. Multi-head: several attention in parallel ‚Üí different relationship types. Encoder: self-attention + feed-forward. Decoder: masked self-attention (sees only past) + cross-attention to encoder. Positional encoding adds position information.',
      },
    },
  },
  {
    id: 'attention',
    position: { x: 700, y: 520 },
    type: 'custom',
    data: {
      emoji: 'üëÅÔ∏è',
      level: 'algorithm',
      ru: {
        label: 'Attention',
        description: '–ú–µ—Ö–∞–Ω–∏–∑–º, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –≤—Ö–æ–¥–∞.',
        keyPoints: [
          'üéØ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞ –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö',
          'üîó –°–≤—è–∑—ã–≤–∞–µ—Ç –ª—é–±—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞–ø—Ä—è–º—É—é',
          'üìä Softmax –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤–µ—Å–∞ –≤ —Å—É–º–º—É = 1',
          'üí° –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –¥–ª—è seq2seq translation',
        ],
        howItWorks: '–ë–∞–∑–æ–≤–∞—è –∏–¥–µ—è: –Ω–µ –≤—Å–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–æ –≤–∞–∂–Ω—ã –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –≤—ã—Ö–æ–¥–∞. Score: –Ω–∞—Å–∫–æ–ª—å–∫–æ Query –ø–æ—Ö–æ–∂ –Ω–∞ –∫–∞–∂–¥—ã–π Key (dot product –∏–ª–∏ MLP). Softmax: –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç scores –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏. Output: –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ Values. –ü—Ä–∏–º–µ—Ä –≤ –ø–µ—Ä–µ–≤–æ–¥–µ: –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–æ–≤–∞ "chat" —Å–º–æ—Ç—Ä–∏–º —Å–∏–ª—å–Ω–µ–µ –Ω–∞ "–∫–æ—Ç" –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –í–∞—Ä–∏–∞–Ω—Ç—ã: additive (Bahdanau), multiplicative (Luong), scaled dot-product (Transformer). Cross-attention: Query –∏–∑ decoder, Key/Value –∏–∑ encoder.',
      },
      en: {
        label: 'Attention',
        description: 'Mechanism allowing model to focus on relevant parts of input.',
        keyPoints: [
          'üéØ Dynamic weights instead of fixed',
          'üîó Connects any positions directly',
          'üìä Softmax normalizes weights to sum = 1',
          'üí° Originally for seq2seq translation',
        ],
        howItWorks: 'Basic idea: not all input parts equally important for current output. Score: how similar Query is to each Key (dot product or MLP). Softmax: turns scores into probabilities. Output: weighted sum of Values. Translation example: when generating "cat" we look more at "–∫–æ—Ç" in source sentence. Variants: additive (Bahdanau), multiplicative (Luong), scaled dot-product (Transformer). Cross-attention: Query from decoder, Key/Value from encoder.',
      },
    },
  },
  {
    id: 'gan',
    position: { x: 880, y: 400 },
    type: 'custom',
    data: {
      emoji: 'üé≠',
      level: 'algorithm',
      ru: {
        label: 'GAN',
        description: 'Generative Adversarial Network ‚Äî –¥–≤–µ —Å–µ—Ç–∏ —Å–æ—Ä–µ–≤–Ω—É—é—Ç—Å—è: –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞—ë—Ç, –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä –ø—Ä–æ–≤–µ—Ä—è–µ—Ç.',
        keyPoints: [
          'üé® Generator —Å–æ–∑–¥–∞—ë—Ç —Ñ–µ–π–∫–∏ –∏–∑ —à—É–º–∞',
          'üîç Discriminator –æ—Ç–ª–∏—á–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ –æ—Ç —Ñ–µ–π–∫–∞',
          '‚öîÔ∏è –ò–≥—Ä–∞ —Å –Ω—É–ª–µ–≤–æ–π —Å—É–º–º–æ–π',
          'üñºÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, deepfakes',
        ],
        howItWorks: 'Generator G(z) –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º z –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. Discriminator D(x) –≤—ã–¥–∞—ë—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —á—Ç–æ x —Ä–µ–∞–ª—å–Ω–æ–µ. –û–±—É—á–µ–Ω–∏–µ: D —É—á–∏—Ç—Å—è –æ—Ç–ª–∏—á–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –æ—Ç G(z), G —É—á–∏—Ç—Å—è –æ–±–º–∞–Ω—ã–≤–∞—Ç—å D. Minimax –∏–≥—Ä–∞: minG maxD [E[log D(x)] + E[log(1-D(G(z)))]]. –ü—Ä–æ–±–ª–µ–º—ã: mode collapse (G –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ), –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. –£–ª—É—á—à–µ–Ω–∏—è: DCGAN, WGAN (Wasserstein distance), StyleGAN (–∫–æ–Ω—Ç—Ä–æ–ª—å —Å—Ç–∏–ª—è).',
      },
      en: {
        label: 'GAN',
        description: 'Generative Adversarial Network ‚Äî two networks compete: generator creates, discriminator verifies.',
        keyPoints: [
          'üé® Generator creates fakes from noise',
          'üîç Discriminator distinguishes real from fake',
          '‚öîÔ∏è Zero-sum game',
          'üñºÔ∏è Image generation, deepfakes',
        ],
        howItWorks: 'Generator G(z) transforms random noise z into image. Discriminator D(x) outputs probability x is real. Training: D learns to distinguish real from G(z), G learns to fool D. Minimax game: minG maxD [E[log D(x)] + E[log(1-D(G(z)))]]. Problems: mode collapse (G generates same thing), training instability. Improvements: DCGAN, WGAN (Wasserstein distance), StyleGAN (style control).',
      },
    },
  },
  {
    id: 'vae',
    position: { x: 880, y: 520 },
    type: 'custom',
    data: {
      emoji: 'üé≤',
      level: 'algorithm',
      ru: {
        label: 'VAE',
        description: 'Variational Autoencoder ‚Äî —É—á–∏—Ç —Å–∂–∞—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö + –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ.',
        keyPoints: [
          'üì¶ Encoder —Å–∂–∏–º–∞–µ—Ç –≤ latent space',
          'üì§ Decoder –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∏–∑ latent',
          'üéØ Latent = —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (Œº, œÉ)',
          'üîÄ –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏',
        ],
        howItWorks: 'Encoder –≤—ã–¥–∞—ë—Ç –Ω–µ —Ç–æ—á–∫—É, –∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: Œº (—Å—Ä–µ–¥–Ω–µ–µ) –∏ œÉ (–¥–∏—Å–ø–µ—Ä—Å–∏—è). Reparametrization trick: z = Œº + œÉ¬∑Œµ, –≥–¥–µ Œµ ~ N(0,1) ‚Äî –ø–æ–∑–≤–æ–ª—è–µ—Ç backprop. Decoder –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç x –∏–∑ z. Loss = reconstruction_loss + KL_divergence(q(z|x) || p(z)). KL –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç latent –±—ã—Ç—å –±–ª–∏–∑–∫–∏–º –∫ N(0,1) ‚Üí –º–æ–∂–Ω–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å –∏–∑ N(0,1) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. Latent space –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π ‚Üí –º–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏.',
      },
      en: {
        label: 'VAE',
        description: 'Variational Autoencoder ‚Äî learns compressed representation + generates new samples.',
        keyPoints: [
          'üì¶ Encoder compresses to latent space',
          'üì§ Decoder reconstructs from latent',
          'üéØ Latent = distribution (Œº, œÉ)',
          'üîÄ Sampling for generation',
        ],
        howItWorks: 'Encoder outputs not point but distribution: Œº (mean) and œÉ (variance). Reparametrization trick: z = Œº + œÉ¬∑Œµ, where Œµ ~ N(0,1) ‚Äî enables backprop. Decoder reconstructs x from z. Loss = reconstruction_loss + KL_divergence(q(z|x) || p(z)). KL forces latent to be close to N(0,1) ‚Üí can sample from N(0,1) for generation. Latent space is continuous ‚Üí can interpolate between objects.',
      },
    },
  },
  {
    id: 'diffusion',
    position: { x: 1060, y: 400 },
    type: 'custom',
    data: {
      emoji: 'üå´Ô∏è',
      level: 'algorithm',
      ru: {
        label: 'Diffusion',
        description: '–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ ‚Äî —É—á–∞—Ç—Å—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–±–∏—Ä–∞—Ç—å —à—É–º, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –¥–∞–Ω–Ω—ã–µ –∏–∑ —á–∏—Å—Ç–æ–≥–æ —à—É–º–∞.',
        keyPoints: [
          '‚û°Ô∏è Forward: –¥–æ–±–∞–≤–ª—è–µ–º —à—É–º —à–∞–≥ –∑–∞ —à–∞–≥–æ–º',
          '‚¨ÖÔ∏è Reverse: —É–±–∏—Ä–∞–µ–º —à—É–º (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è)',
          'üé® SOTA –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π',
          'üñºÔ∏è Stable Diffusion, DALL-E, Midjourney',
        ],
        howItWorks: 'Forward process: x‚ÇÄ ‚Üí x‚ÇÅ ‚Üí ... ‚Üí x‚Çú, –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–æ–±–∞–≤–ª—è–µ–º –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º, –≤ –∫–æ–Ω—Ü–µ ‚Äî —á–∏—Å—Ç—ã–π —à—É–º. Reverse process: –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —à—É–º –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —à–∞–≥–µ t, –∏ –≤—ã—á–∏—Ç–∞–µ—Ç –µ–≥–æ: x‚Çú ‚Üí x‚Çú‚Çã‚ÇÅ ‚Üí ... ‚Üí x‚ÇÄ. –û–±—É—á–µ–Ω–∏–µ: –±–µ—Ä—ë–º —á–∏—Å—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º —à—É–º, –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —ç—Ç–æ—Ç —à—É–º. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –Ω–∞—á–∏–Ω–∞–µ–º —Å —á–∏—Å—Ç–æ–≥–æ —à—É–º–∞, –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–±–∏—Ä–∞–µ–º. –£—Å–ª–æ–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π embedding (CLIP) –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è.',
      },
      en: {
        label: 'Diffusion',
        description: 'Diffusion models ‚Äî learn to gradually remove noise, generating data from pure noise.',
        keyPoints: [
          '‚û°Ô∏è Forward: add noise step by step',
          '‚¨ÖÔ∏è Reverse: remove noise (generation)',
          'üé® SOTA for image generation',
          'üñºÔ∏è Stable Diffusion, DALL-E, Midjourney',
        ],
        howItWorks: 'Forward process: x‚ÇÄ ‚Üí x‚ÇÅ ‚Üí ... ‚Üí x‚Çú, at each step add Gaussian noise, end with pure noise. Reverse process: model learns to predict noise added at step t, and subtracts it: x‚Çú ‚Üí x‚Çú‚Çã‚ÇÅ ‚Üí ... ‚Üí x‚ÇÄ. Training: take clean image, add noise, model predicts this noise. Generation: start from pure noise, iteratively remove. Conditional generation: add text embedding (CLIP) for control.',
      },
    },
  },

  // ========== IMPLEMENTATION (NLP) ==========
  {
    id: 'llm',
    position: { x: 950, y: 260 },
    type: 'custom',
    data: {
      emoji: 'üó£Ô∏è',
      level: 'implementation',
      ru: {
        label: 'LLM',
        description: '–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ ‚Äî –º–∞—Å—à—Ç–∞–±–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ—Ä–ø—É—Å–∞—Ö.',
        keyPoints: [
          'üìè –†–∞–∑–º–µ—Ä: –º–∏–ª–ª–∏–∞—Ä–¥—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (GPT-4: ~1.7T)',
          'üìö –û–±—É—á–µ–Ω–∏–µ: –≤–µ—Å—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç + –∫–Ω–∏–≥–∏ + –∫–æ–¥',
          'üé≠ Emergent abilities –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏',
          'üîß –ü—Ä–∏–º–µ—Ä—ã: GPT-4, Claude, Gemini, LLaMA',
        ],
        howItWorks: '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: decoder-only transformer. –û–±—É—á–µ–Ω–∏–µ: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ (causal LM). –î–∞—Ç–∞—Å–µ—Ç: —Ç—Ä–∏–ª–ª–∏–æ–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤ —Ç–µ–∫—Å—Ç–∞. Scaling laws: –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ + –¥–∞–Ω–Ω—ã—Ö = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ. Emergent abilities: –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ –ø–æ—è–≤–ª—è—é—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ (in-context learning, reasoning). Fine-tuning: RLHF (–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö) –¥–ª—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. Inference: –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º.',
      },
      en: {
        label: 'LLM',
        description: 'Large Language Models ‚Äî massive transformers trained on huge text corpora.',
        keyPoints: [
          'üìè Size: billions of parameters (GPT-4: ~1.7T)',
          'üìö Training: entire internet + books + code',
          'üé≠ Emergent abilities at scale',
          'üîß Examples: GPT-4, Claude, Gemini, LLaMA',
        ],
        howItWorks: 'Architecture: decoder-only transformer. Training: next token prediction (causal LM). Dataset: trillions of text tokens. Scaling laws: more parameters + data = better quality. Emergent abilities: at certain scale new capabilities appear (in-context learning, reasoning). Fine-tuning: RLHF (learning from human preferences) for instruction following. Inference: autoregressive generation token by token.',
      },
    },
  },
  {
    id: 'embeddings',
    position: { x: 1130, y: 260 },
    type: 'custom',
    data: {
      emoji: 'üìê',
      level: 'implementation',
      ru: {
        label: 'Embeddings',
        description: '–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è ‚Äî —Å–ª–æ–≤–∞/—Ç–µ–∫—Å—Ç—ã –∫–∞–∫ —Ç–æ—á–∫–∏ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.',
        keyPoints: [
          'üìä –í–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (768, 1536...)',
          'üìè –ü–æ—Ö–æ–∂–∏–µ –ø–æ–Ω—è—Ç–∏—è = –±–ª–∏–∑–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã',
          '‚ûï –ê—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞: king - man + woman ‚âà queen',
          'üîç –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: –ø–æ–∏—Å–∫, RAG, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è',
        ],
        howItWorks: 'Word2Vec: –æ–±—É—á–∞–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–æ–≤–æ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (CBOW) –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —Å–ª–æ–≤—É (Skip-gram). Sentence embeddings: —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (SBERT). –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: cos(a,b) = (a¬∑b)/(|a||b|) ‚Äî –º–µ—Ä–∞ –±–ª–∏–∑–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ RAG: —Ç–µ–∫—Å—Ç ‚Üí embedding ‚Üí –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –≤ –±–∞–∑–µ ‚Üí –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM. OpenAI, Cohere, Voyage –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç embedding API. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é.',
      },
      en: {
        label: 'Embeddings',
        description: 'Vector representations ‚Äî words/texts as points in multidimensional space.',
        keyPoints: [
          'üìä Fixed dimension vector (768, 1536...)',
          'üìè Similar concepts = nearby vectors',
          '‚ûï Arithmetic: king - man + woman ‚âà queen',
          'üîç Uses: search, RAG, clustering',
        ],
        howItWorks: 'Word2Vec: learns to predict word from context (CBOW) or context from word (Skip-gram). Sentence embeddings: average tokens or specialized model (SBERT). Cosine similarity: cos(a,b) = (a¬∑b)/(|a||b|) ‚Äî vector closeness measure. RAG application: text ‚Üí embedding ‚Üí search similar in database ‚Üí context for LLM. OpenAI, Cohere, Voyage provide embedding APIs. Dimensionality: tradeoff between quality and speed.',
      },
    },
  },
  {
    id: 'tokenization',
    position: { x: 1130, y: 380 },
    type: 'custom',
    data: {
      emoji: '‚úÇÔ∏è',
      level: 'implementation',
      ru: {
        label: '–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è',
        description: '–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã (–µ–¥–∏–Ω–∏—Ü—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏).',
        keyPoints: [
          '‚úÇÔ∏è –¢–µ–∫—Å—Ç ‚Üí —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ (subwords)',
          'üìñ –°–ª–æ–≤–∞—Ä—å: 30k-100k —Ç–æ–∫–µ–Ω–æ–≤',
          'üî§ BPE, WordPiece, SentencePiece –∞–ª–≥–æ—Ä–∏—Ç–º—ã',
          'üí∞ –í–ª–∏—è–µ—Ç –Ω–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å API ($/1M tokens)',
        ],
        howItWorks: 'BPE (Byte Pair Encoding): –Ω–∞—á–∏–Ω–∞–µ—Ç —Å —Å–∏–º–≤–æ–ª–æ–≤, –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –ø–∞—Ä—ã. "tokenization" ‚Üí ["token", "ization"] –∏–ª–∏ ["to", "ken", "iz", "ation"]. –†–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞ —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞, —á–∞—Å—Ç—ã–µ –æ—Å—Ç–∞—é—Ç—Å—è —Ü–µ–ª—ã–º–∏. –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: [CLS], [SEP], [PAD], <|endoftext|>. –ü—Ä–æ–±–ª–µ–º—ã: —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É, —ç–º–æ–¥–∑–∏ –∏ –∫–æ–¥ –º–æ–≥—É—Ç –∑–∞–Ω–∏–º–∞—Ç—å –º–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤. tiktoken (OpenAI) ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤.',
      },
      en: {
        label: 'Tokenization',
        description: 'Splitting text into tokens (processing units for the model).',
        keyPoints: [
          '‚úÇÔ∏è Text ‚Üí list of tokens (subwords)',
          'üìñ Vocabulary: 30k-100k tokens',
          'üî§ BPE, WordPiece, SentencePiece algorithms',
          'üí∞ Affects API cost ($/1M tokens)',
        ],
        howItWorks: 'BPE (Byte Pair Encoding): starts with characters, iteratively merges most frequent pairs. "tokenization" ‚Üí ["token", "ization"] or ["to", "ken", "iz", "ation"]. Rare words split into subwords, frequent stay whole. Special tokens: [CLS], [SEP], [PAD], <|endoftext|>. Issues: different languages tokenize differently, emoji and code may take many tokens. tiktoken (OpenAI) ‚Äî library for counting tokens.',
      },
    },
  },
];

// ==================== EDGES ====================

export const initialEdges: Edge[] = [
  // AI ‚Üí Theories
  { id: 'ai-ml', source: 'ai', target: 'ml', animated: true },
  { id: 'ai-dl', source: 'ai', target: 'dl', animated: true },
  { id: 'ai-nlp', source: 'ai', target: 'nlp', animated: true },
  
  // ML ‚Üí Methods
  { id: 'ml-sup', source: 'ml', target: 'supervised' },
  { id: 'ml-unsup', source: 'ml', target: 'unsupervised' },
  { id: 'ml-rl', source: 'ml', target: 'rl' },
  
  // Supervised ‚Üí Algorithms
  { id: 'sup-linreg', source: 'supervised', target: 'linear-reg' },
  { id: 'sup-tree', source: 'supervised', target: 'decision-tree' },
  { id: 'sup-rf', source: 'supervised', target: 'random-forest' },
  { id: 'sup-svm', source: 'supervised', target: 'svm' },
  
  // Unsupervised ‚Üí Algorithms
  { id: 'unsup-kmeans', source: 'unsupervised', target: 'kmeans' },
  { id: 'unsup-pca', source: 'unsupervised', target: 'pca' },
  
  // RL ‚Üí Algorithms
  { id: 'rl-qlearn', source: 'rl', target: 'qlearning' },
  
  // DL ‚Üí Architectures
  { id: 'dl-nn', source: 'dl', target: 'nn' },
  { id: 'dl-cnn', source: 'dl', target: 'cnn' },
  { id: 'dl-rnn', source: 'dl', target: 'rnn' },
  { id: 'rnn-lstm', source: 'rnn', target: 'lstm' },
  { id: 'dl-trans', source: 'dl', target: 'transformer' },
  { id: 'trans-attn', source: 'transformer', target: 'attention' },
  { id: 'dl-gan', source: 'dl', target: 'gan' },
  { id: 'dl-vae', source: 'dl', target: 'vae' },
  { id: 'dl-diff', source: 'dl', target: 'diffusion' },
  
  // NLP ‚Üí Implementations
  { id: 'nlp-llm', source: 'nlp', target: 'llm' },
  { id: 'nlp-emb', source: 'nlp', target: 'embeddings' },
  { id: 'nlp-tok', source: 'nlp', target: 'tokenization' },
  
  // Cross-connections (dashed = —Å–≤—è–∑—å –º–µ–∂–¥—É –≤–µ—Ç–∫–∞–º–∏)
  { id: 'trans-llm', source: 'transformer', target: 'llm', style: { strokeDasharray: '5,5' } },
  { id: 'sup-nn', source: 'supervised', target: 'nn', style: { strokeDasharray: '5,5' } },
  { id: 'tree-rf', source: 'decision-tree', target: 'random-forest', style: { strokeDasharray: '5,5' } },
  { id: 'attn-llm', source: 'attention', target: 'llm', style: { strokeDasharray: '5,5' } },
];
